<h1 class="elementor-heading-title elementor-size-default"><span style="color: #000000;"><a style="color: #000000;" href="http://www.datatreeresearch.com/simple-understanding-of-generalized-ensemble-stacking-with-cross-validation/">Simple Understanding of Generalized Ensemble / Stacking with Cross Validation</a></span></h1>

I was recently going through an excellent blog called <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://mlwave.com/kaggle-ensembling-guide/" target="_blank" rel="noopener">Kaggle Ensembling Guide</a></span>. Based on the explanations and code available on the blog, I have created a simple Algorithm to explain the Generalized Stacking Algorithm.

<p>Lets say we have a small Train data set of 12 rows, 2 independent and one dependent variable. And a test set of 4 observations.</p>


<figure class="wp-block-image size-large"><img src="http://www.datatreeresearch.com/wp-content/uploads/2020/02/3-Fold-Cross-Validation.jpg" alt="" class="wp-image-4776"/></figure>

<p>We decide to do a 3-Fold Cross Validation (totally random choice). For sake of simplicity we choose a 2 layer stacking with 2 base models and 1 meta learner model.</p>

<div class="wp-block-image"><figure class="aligncenter size-large">
<img src="http://www.datatreeresearch.com/wp-content/uploads/2020/02/Ensemble-Stacking-Architecture.jpg" 
alt="" class="wp-image-4781"/></figure></div>

The Algorithm simulation is roughly as follows. 

<div class="wp-block-image"><figure class="aligncenter size-large"><img src="http://www.datatreeresearch.com/wp-content/uploads/2020/02/Generalized-Stacking-Blending.gif" alt="" class="wp-image-4782"/></figure></div>


<p>The &ldquo;Steps&rdquo; mentioned below match the title of the simulation images.</p>
<ul>
<li>Step 1-3: For Model 1, For the k'th split in the train data, fit each model into all but kth fold and predict the training column for kth fold.
<ul>
<li>Also build a test column for the full test data using the same fitted model at each step</li>
</ul>
</li>
<li>Step 4: Do this for all folds to fit the training &amp; test data completely and then average out all the test columns to build a single model column</li>
<li>Step 5-8: Repeat the above steps(1-4) are carried for Model 2, we will get no of column predictors for test data as a number of models chosen. Similarly at each point, build the test prediction for Model 2. In the end, average all predicted columns for Test data to build the final predictor for Model 2 for the Test data.
<ul>
<li>The Train and Test data both will now have 2 predictor columns (XM1 and XM2) corresponding to Model1 and Model2</li>
</ul>
</li>
<li>Step 9: Then apply the next level stacking using Model3 to fit the (full) train data (Y ~ XM1 + XM2) on the derived columns and output (completely). The use this model to predict for the Test data &ndash; making the overall final prediction.</li>
</ul>
<p>&nbsp;</p>
<p>Example: Kaggle <a href="https://www.kaggle.com/c/tut-head-pose-estimation-challenge" target="_blank" rel="noopener"><span style="color: #0000ff;"><strong>TUT Head Pose Estimation Challenge</strong></span></a></p>

<p>The challenge is to predict the direction of eyes (angles for both eyes) using the data given (there are 2 Y variables). I have written a sample code using 2 level Ensemble stacking architecture as discussed above. The only differences are:</p>
<ul>
<li>There are 2 Y variables, hence the process of modelling and prediction is repeated twice</li>
<li>I have used 5 models in Level 1 and 1 Model in Level 2</li>
<li>K = 10</li>
</ul>

<div style="background-color: #eee; color: #000000; padding: 5px; border: 1px solid #999; font-family: Courier New; font-size: small;">
<p><strong>import </strong>numpy <strong>as </strong>np<br /> <strong>from </strong>sklearn.model_selection <strong>import </strong>StratifiedKFold<br /> <strong>from </strong>sklearn.ensemble <strong>import </strong>RandomForestClassifier, ExtraTreesClassifier<br /> <strong>from </strong>sklearn.ensemble <strong>import </strong>GradientBoostingClassifier<br /> <strong>from </strong>sklearn.linear_model <strong>import </strong>LogisticRegression<br /> <strong>import </strong>os<br /> <strong>import </strong>pandas <strong>as </strong>pd<br /> <strong>from </strong>sklearn <strong>import </strong>ensemble<br /> <br /> <strong>import </strong>os<br /> cwd <strong>= </strong>os.getcwd<strong>()<br /> <br /> </strong>loc_train_x <strong>= </strong>cwd <strong>+ "/Input CSVs/tut-head-pose-estimation-challenge/X_train.csv"<br /> </strong>loc_train_y <strong>= </strong>cwd <strong>+ "/Input CSVs/tut-head-pose-estimation-challenge/y_train.csv"<br /> </strong>loc_test <strong>= </strong>cwd <strong>+ "/Input CSVs/tut-head-pose-estimation-challenge/X_test.csv"<br /> </strong>loc_sample_submission <strong>= </strong>cwd <strong>+ "/Input CSVs/tut-head-pose-estimation-challenge/sample_submission.csv"<br /> <br /> </strong>X_train <strong>= </strong>pd.read_csv<strong>(</strong>loc_train_x<strong>)<br /> </strong>y_train <strong>= </strong>pd.read_csv<strong>(</strong>loc_train_y<strong>)<br /> </strong>X_test <strong>= </strong>pd.read_csv<strong>(</strong>loc_test<strong>)<br /> </strong>X_submission <strong>= </strong>pd.read_csv<strong>(</strong>loc_sample_submission<strong>)<br /> <br /> </strong>feature_cols <strong>= [</strong>col <strong>for </strong>col <strong>in </strong>X_train.columns <strong>if </strong>col <strong>not in ["Id"]]<br /> </strong>X <strong>= </strong>X_train<strong>[</strong>feature_cols<strong>]</strong>.values<br /> y1 <strong>= </strong>y_train<strong>["Angle1"]</strong>.values<br /> y2 <strong>= </strong>y_train<strong>["Angle2"]</strong>.values<br /> X_test <strong>= </strong>X_test<strong>[</strong>feature_cols<strong>]</strong>.values<br /> <br /> <br /> clfs <strong>= [</strong>RandomForestClassifier<strong>(</strong>n_estimators<strong>=</strong>500, n_jobs<strong>=-</strong>1, criterion<strong>='gini')</strong>,<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RandomForestClassifier<strong>(</strong>n_estimators<strong>=</strong>500, n_jobs<strong>=-</strong>1, criterion<strong>='entropy')</strong>,<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ExtraTreesClassifier<strong>(</strong>n_estimators<strong>=</strong>500, n_jobs<strong>=-</strong>1, criterion<strong>='gini')</strong>,<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ExtraTreesClassifier<strong>(</strong>n_estimators<strong>=</strong>500, n_jobs<strong>=-</strong>1, criterion<strong>='entropy')</strong>,<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; GradientBoostingClassifier<strong>(</strong>learning_rate<strong>=</strong>0.05, subsample<strong>=</strong>0.5, max_depth<strong>=</strong>6, n_estimators<strong>=</strong>50<strong>)]<br /> <br /> <br /> </strong><br /> <span style="color: #0000ff;">#For using Cross-validated</span><br /> n_folds <strong>= </strong>10<br /> shuffle <strong>= </strong>False<br /> <strong>from </strong>sklearn.model_selection <strong>import </strong>StratifiedKFold<br /> skf <strong>= </strong>StratifiedKFold<strong>(</strong>n_splits<strong>=</strong>n_folds, shuffle<strong>=</strong>shuffle<strong>)<br /> </strong>skf.get_n_splits<strong>(</strong>X, y1<strong>)<br /> <br /> </strong>dataset_blend_train <strong>= </strong>np.zeros<strong>((</strong>X.shape<strong>[</strong>0<strong>]</strong>, len<strong>(</strong>clfs<strong>)))<br /> </strong>dataset_blend_test <strong>= </strong>np.zeros<strong>((</strong>X_test.shape<strong>[</strong>0<strong>]</strong>, len<strong>(</strong>clfs<strong>)))<br /> <br /> </strong>loc_temp_save_base <strong>= </strong>cwd <strong>+ "/Input CSVs/tut-head-pose-estimation-challenge/"<br /> <br /> </strong></p>
<p><span style="color: #0000ff;">#There are logs below just in case someone wants to understand what is actually going on and how this Stacking Algo works.</span><br /> <br /><span style="color: #0000ff;"> #We have 2 Y vars to be predicted</span><br /> y_list <strong>= [</strong>y1, y2<strong>]<br /> <br /> </strong></p>
<p><strong>for </strong>e, y <strong>in </strong>enumerate<strong>(</strong>y_list<strong>):<br /> &nbsp;&nbsp;&nbsp; for </strong>j, clf <strong>in </strong>enumerate<strong>(</strong>clfs<strong>):<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print </strong>j, clf<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; dataset_blend_test_j <strong>= </strong>np.zeros<strong>((</strong>X_test.shape<strong>[</strong>0<strong>]</strong>, <strong>(</strong>skf.n_splits<strong>)))<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong>i <strong>= </strong>0<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>for </strong>train_index, test_index <strong>in </strong>skf.split<strong>(</strong>X, y<strong>):<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print "Fold"</strong>, i<br /> <strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong>X_train, X_train_test <strong>= </strong>X<strong>[</strong>train_index<strong>]</strong>, X<strong>[</strong>test_index<strong>]<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong>y_train, y_test <strong>= </strong>y<strong>[</strong>train_index<strong>]</strong>, y<strong>[</strong>test_index<strong>]<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong>clf.fit<strong>(</strong>X_train, y_train<strong>)<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong>y_submission <strong>= </strong>clf.predict<strong>(</strong>X_train_test<strong>)<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong>dataset_blend_train<strong>[</strong>test_index, j<strong>] = </strong>y_submission<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color: #0000ff;">#Temp logging</span><br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loc_temp_save <strong>= </strong>loc_temp_save_base <strong>+ "dataset_blend_train_" + </strong>str<strong>(</strong>clf<strong>)</strong>.split<strong>('(')[</strong>0<strong>] + </strong>str<strong>(</strong>j<strong>) + </strong>str<strong>(</strong>i<strong>) + ".csv"<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong>pd.DataFrame<strong>(</strong>dataset_blend_train<strong>)</strong>.to_csv<strong>(</strong>loc_temp_save<strong>)<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong><span style="color: #0000ff;"># print "dataset_blend_train"</span><br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color: #0000ff;"># print dataset_blend_train</span><br /> <br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; dataset_blend_test_j<strong>[:</strong>, i<strong>] = </strong>clf.predict<strong>(</strong>X_test<strong>)<br /> </strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #Temp logging<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loc_temp_save <strong>= </strong>loc_temp_save_base <strong>+ "dataset_blend_test_j_" + </strong>str<strong>(</strong>clf<strong>)</strong>.split<strong>('(')[</strong>0<strong>] + </strong>str<strong>(</strong>j<strong>) + </strong>str<strong>(</strong>i<strong>) + ".csv"<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong>pd.DataFrame<strong>(</strong>dataset_blend_test_j<strong>)</strong>.to_csv<strong>(</strong>loc_temp_save<strong>)<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong>i <strong>+= </strong>1<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; dataset_blend_test<strong>[:</strong>, j<strong>] = </strong>dataset_blend_test_j.mean<strong>(</strong>axis<strong>=</strong>1<strong>)<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong><span style="color: #0000ff;"># Temp logging</span><br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loc_temp_save <strong>= </strong>loc_temp_save_base <strong>+ "dataset_blend_test_" + </strong>str<strong>(</strong>clf<strong>)</strong>.split<strong>('(')[</strong>0<strong>] + </strong>str<strong>(</strong>j<strong>) + ".csv"<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong>pd.DataFrame<strong>(</strong>dataset_blend_test<strong>)</strong>.to_csv<strong>(</strong>loc_temp_save<strong>)<br /> &nbsp;&nbsp;&nbsp; print "Level 2 Fitting"<br /> &nbsp;&nbsp;&nbsp; </strong>clf <strong>= </strong>ExtraTreesClassifier<strong>(</strong>n_estimators<strong>=</strong>500, n_jobs<strong>=-</strong>1, criterion<strong>='entropy')<br /> &nbsp;&nbsp;&nbsp; </strong>clf.fit<strong>(</strong>dataset_blend_train, y<strong>)<br /> &nbsp;&nbsp;&nbsp; </strong>y_submission <strong>= </strong>clf.predict<strong>(</strong>dataset_blend_test<strong>)<br /> &nbsp;&nbsp;&nbsp; </strong><span style="color: #0000ff;"># Assign values to a column in submission dataframe</span><br /> &nbsp;&nbsp;&nbsp; X_submission.iloc<strong>[:</strong>, <strong>[</strong>e <strong>+ </strong>1<strong>]] = </strong>np.reshape<strong>(</strong>y_submission,<strong>(-</strong>1,1<strong>))<br /> <br /> </strong><span style="color: #0000ff;">#Submission File</span></p>
<p>loc_actual_submission <strong>= </strong>cwd <strong>+ "/Input CSVs/tut-head-pose-estimation-challenge/actual_submission.csv"<br /> </strong>X_submission.to_csv<strong>(</strong>loc_actual_submission, index<strong>=</strong>False<strong>)</strong></p>
<p>&nbsp;</p>
</div>
<p>The output gets an error of close to 5.5 and can easily put you in Top 15. Please do let me know if you found this useful.</p>
<p>Reach out to me on: <a href="mailto:parijat@datatreeresearch.com">parijat@datatreeresearch.com</a></p>